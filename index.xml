<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>KubeFleet</title><link>https://kubefleet-dev.github.io/website/</link><description>Recent content on KubeFleet</description><generator>Hugo</generator><language>en</language><atom:link href="https://kubefleet-dev.github.io/website/index.xml" rel="self" type="application/rss+xml"/><item><title>ClusterResourcePlacement TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacement/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacement/</guid><description>&lt;p>This TSG is meant to help you troubleshoot issues with the ClusterResourcePlacement API in Fleet.&lt;/p>
&lt;h2 id="cluster-resource-placement">Cluster Resource Placement&lt;/h2>
&lt;p>Internal Objects to keep in mind when troubleshooting CRP related errors on the hub cluster:&lt;/p>
&lt;ul>
&lt;li>&lt;code>ClusterResourceSnapshot&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterSchedulingPolicySnapshot&lt;/code>&lt;/li>
&lt;li>&lt;code>ClusterResourceBinding&lt;/code>&lt;/li>
&lt;li>&lt;code>Work&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Please read the &lt;a href="https://kubefleet-dev.github.io/website/docs/api-reference/">Fleet API reference&lt;/a> for more details about each object.&lt;/p>
&lt;h2 id="complete-progress-of-the-clusterresourceplacement">Complete Progress of the ClusterResourcePlacement&lt;/h2>
&lt;p>Understanding the progression and the status of the &lt;code>ClusterResourcePlacement&lt;/code> custom resource is crucial for diagnosing and identifying failures.
You can view the status of the &lt;code>ClusterResourcePlacement&lt;/code> custom resource by using the following command:&lt;/p></description></item><item><title>Fleet components</title><link>https://kubefleet-dev.github.io/website/docs/concepts/components/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/components/</guid><description>&lt;h1 id="components">Components&lt;/h1>
&lt;p>This document provides an overview of the components required for a fully functional and operational Fleet setup.&lt;/p>
&lt;p>&lt;img src="https://kubefleet-dev.github.io/website/images/en/docs/concepts/components/architecture.jpg" alt="">&lt;/p>
&lt;p>The fleet consists of the following components:&lt;/p>
&lt;ul>
&lt;li>fleet-hub-agent is a Kubernetes controller that creates and reconciles all the fleet related CRs in the hub cluster.&lt;/li>
&lt;li>fleet-member-agent is a Kubernetes controller that creates and reconciles all the fleet related CRs in the member cluster.
The fleet-member-agent is pulling the latest CRs from the hub cluster and consistently reconciles the member clusters to
the desired state.&lt;/li>
&lt;/ul>
&lt;p>The fleet implements agent-based pull mode. So that the working pressure can be distributed to the member clusters, and it
helps to breach the bottleneck of scalability, by dividing the load into each member cluster. On the other hand, hub
cluster does not need to directly access to the member clusters. Fleet can support the member clusters which only have
the outbound network and no inbound network access.&lt;/p></description></item><item><title>Resource Migration Across Clusters</title><link>https://kubefleet-dev.github.io/website/docs/tutorials/clustermigrationdr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/tutorials/clustermigrationdr/</guid><description>&lt;p>This tutorial demonstrates how to move applications from clusters have gone down to other operational clusters using Fleet.&lt;/p>
&lt;h2 id="scenario">Scenario&lt;/h2>
&lt;p>Your fleet consists of the following clusters:&lt;/p>
&lt;ol>
&lt;li>Member Cluster 1 &amp;amp; Member Cluster 2 (WestUS, 1 node each)&lt;/li>
&lt;li>Member Cluster 3 (EastUS2, 2 nodes)&lt;/li>
&lt;li>Member Cluster 4 &amp;amp; Member Cluster 5 (WestEurope, 3 nodes each)&lt;/li>
&lt;/ol>
&lt;p>Due to certain circumstances, Member Cluster 1 and Member Cluster 2 are down, requiring you to migrate your applications from these clusters to other operational ones.&lt;/p></description></item><item><title>CRP Schedule Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementscheduled/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementscheduled/</guid><description>&lt;p>The &lt;code>ClusterResourcePlacementScheduled&lt;/code> condition is set to &lt;code>false&lt;/code> when the scheduler cannot find all the clusters needed as specified by the scheduling policy.&lt;/p>
&lt;blockquote>
&lt;p>Note: To get more information about why the scheduling fails, you can check the &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/scheduler/scheduler.go">scheduler&lt;/a> logs.&lt;/p>&lt;/blockquote>
&lt;h2 id="common-scenarios">Common scenarios&lt;/h2>
&lt;p>Instances where this condition may arise:&lt;/p>
&lt;ul>
&lt;li>When the placement policy is set to &lt;code>PickFixed&lt;/code>, but the specified cluster names do not match any joined member cluster name in the fleet, or the specified cluster is no longer connected to the fleet.&lt;/li>
&lt;li>When the placement policy is set to &lt;code>PickN&lt;/code>, and N clusters are specified, but there are fewer than N clusters that have joined the fleet or satisfy the placement policy.&lt;/li>
&lt;li>When the &lt;code>ClusterResourcePlacement&lt;/code> resource selector selects a reserved namespace.&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Note: When the placement policy is set to &lt;code>PickAll&lt;/code>, the &lt;code>ClusterResourcePlacementScheduled&lt;/code> condition is always set to &lt;code>true&lt;/code>.&lt;/p></description></item><item><title>Managing clusters</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/clusters/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/clusters/</guid><description>&lt;p>This how-to guide discusses how to manage clusters in a fleet, specifically:&lt;/p>
&lt;ul>
&lt;li>how to join a cluster into a fleet; and&lt;/li>
&lt;li>how to set a cluster to leave a fleet; and&lt;/li>
&lt;li>how to add labels to a member cluster&lt;/li>
&lt;/ul>
&lt;h2 id="joining-a-cluster-into-a-fleet">Joining a cluster into a fleet&lt;/h2>
&lt;p>A cluster can join in a fleet if:&lt;/p>
&lt;ul>
&lt;li>it runs a supported Kubernetes version; it is recommended that you use Kubernetes 1.24 or later
versions, and&lt;/li>
&lt;li>it has network connectivity to the hub cluster of the fleet.&lt;/li>
&lt;/ul>
&lt;p>For your convenience, Fleet provides a script that can automate the process of joining a cluster
into a fleet. To use the script, run the commands below:&lt;/p></description></item><item><title>MemberCluster</title><link>https://kubefleet-dev.github.io/website/docs/concepts/membercluster/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/membercluster/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The fleet constitutes an implementation of a &lt;a href="https://multicluster.sigs.k8s.io/api-types/cluster-set/">&lt;code>ClusterSet&lt;/code>&lt;/a> and
encompasses the following attributes:&lt;/p>
&lt;ul>
&lt;li>A collective of clusters managed by a centralized authority.&lt;/li>
&lt;li>Typically characterized by a high level of mutual trust within the cluster set.&lt;/li>
&lt;li>Embraces the principle of Namespace Sameness across clusters:
&lt;ul>
&lt;li>Ensures uniform permissions and characteristics for a given namespace across all clusters.&lt;/li>
&lt;li>While not mandatory for every cluster, namespaces exhibit consistent behavior across those where they are present.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>The &lt;code>MemberCluster&lt;/code> represents a cluster-scoped API established within the hub cluster, serving as a representation of
a cluster within the fleet. This API offers a dependable, uniform, and automated approach for multi-cluster applications
(frameworks, toolsets) to identify registered clusters within a fleet. Additionally, it facilitates applications in querying
a list of clusters managed by the fleet or observing cluster statuses for subsequent actions.&lt;/p></description></item><item><title>Resource Migration With Overrides</title><link>https://kubefleet-dev.github.io/website/docs/tutorials/migrationwithoverridedr/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/tutorials/migrationwithoverridedr/</guid><description>&lt;p>This tutorial shows how to migrate applications from clusters with lower availability to those with higher availability,
while also scaling up the number of replicas, using Fleet.&lt;/p>
&lt;h2 id="scenario">Scenario&lt;/h2>
&lt;p>Your fleet consists of the following clusters:&lt;/p>
&lt;ol>
&lt;li>Member Cluster 1 &amp;amp; Member Cluster 2 (WestUS, 1 node each)&lt;/li>
&lt;li>Member Cluster 3 (EastUS2, 2 nodes)&lt;/li>
&lt;li>Member Cluster 4 &amp;amp; Member Cluster 5 (WestEurope, 3 nodes each)&lt;/li>
&lt;/ol>
&lt;p>Due to a sudden increase in traffic and resource demands in your WestUS clusters, you need to migrate your applications to clusters in EastUS2 or WestEurope that have higher availability and can better handle the increased load.&lt;/p></description></item><item><title>CRP Rollout Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementrolloutstarted/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementrolloutstarted/</guid><description>&lt;p>When using the &lt;code>ClusterResourcePlacement&lt;/code> API object in Azure Kubernetes Fleet Manager to propagate resources, the selected resources aren&amp;rsquo;t rolled out in all scheduled clusters and the &lt;code>ClusterResourcePlacementRolloutStarted&lt;/code> condition status shows as &lt;code>False&lt;/code>.&lt;/p>
&lt;p>&lt;em>This TSG only applies to the &lt;code>RollingUpdate&lt;/code> rollout strategy, which is the default strategy if you don&amp;rsquo;t specify in the &lt;code>ClusterResourcePlacement&lt;/code>.&lt;/em>
&lt;em>To troubleshoot the update run strategy as you specify &lt;code>External&lt;/code> in the &lt;code>ClusterResourcePlacement&lt;/code>, please refer to the &lt;a href="https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterstagedupdaterun/">Staged Update Run Troubleshooting Guide&lt;/a>.&lt;/em>&lt;/p></description></item><item><title>Getting started with Fleet using KinD clusters</title><link>https://kubefleet-dev.github.io/website/docs/getting-started/kind/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/getting-started/kind/</guid><description>&lt;p>In this tutorial, you will try Fleet out using
&lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> clusters, which are Kubernetes clusters running on your own
local machine via &lt;a href="https://docker.com">Docker&lt;/a> containers. This is the easiest way
to get started with Fleet, which can help you understand how Fleet simiplify the day-to-day multi-cluster management experience with very little setup needed.&lt;/p>
&lt;blockquote>
&lt;p>Note&lt;/p>
&lt;p>kind is a tool for setting up a Kubernetes environment for experimental purposes;
some instructions below for running Fleet in the kind environment may not apply to other
environments, and there might also be some minor differences in the Fleet
experience.&lt;/p></description></item><item><title>KubeFleet and ArgoCD Integration</title><link>https://kubefleet-dev.github.io/website/docs/tutorials/argocd/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/tutorials/argocd/</guid><description>&lt;p>This hands-on guide of KubeFleet and ArgoCD integration shows how these powerful tools work in concert to revolutionize multi-cluster application management.
Discover how KubeFleet&amp;rsquo;s intelligent orchestration capabilities complement ArgoCD&amp;rsquo;s popular GitOps approach, enabling seamless deployments across diverse environments while maintaining consistency and control.
This tutorial illuminates practical strategies for targeted deployments, environment-specific configurations, and safe, controlled rollouts.
Follow along to transform your multi-cluster challenges into streamlined, automated workflows that enhance both developer productivity and operational reliability.&lt;/p></description></item><item><title>Using the ClusterResourcePlacement API</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/crp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/crp/</guid><description>&lt;p>This guide provides an overview of how to use the Fleet &lt;code>ClusterResourcePlacement&lt;/code> (CRP) API to orchestrate workload distribution across your fleet.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The CRP API is a core Fleet API that facilitates the distribution of specific resources from the hub cluster to
member clusters within a fleet. This API offers scheduling capabilities that allow you to target the most suitable
group of clusters for a set of resources using a complex rule set. For example, you can distribute resources to
clusters in specific regions (North America, East Asia, Europe, etc.) and/or release stages (production, canary, etc.).
You can even distribute resources according to certain topology spread constraints.&lt;/p></description></item><item><title>ClusterResourcePlacement</title><link>https://kubefleet-dev.github.io/website/docs/concepts/crp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/crp/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;code>ClusterResourcePlacement&lt;/code> concept is used to dynamically select cluster scoped resources (especially namespaces and all
objects within it) and control how they are propagated to all or a subset of the member clusters.
A &lt;code>ClusterResourcePlacement&lt;/code> mainly consists of three parts:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Resource selection&lt;/strong>: select which cluster-scoped Kubernetes
resource objects need to be propagated from the hub cluster to selected member clusters.&lt;/p>
&lt;p>It supports the following forms of resource selection:&lt;/p>
&lt;ul>
&lt;li>Select resources by specifying just the &amp;lt;group, version, kind&amp;gt;. This selection propagates all resources with matching &amp;lt;group, version, kind&amp;gt;.&lt;/li>
&lt;li>Select resources by specifying the &amp;lt;group, version, kind&amp;gt; and name. This selection propagates only one resource that matches the &amp;lt;group, version, kind&amp;gt; and name.&lt;/li>
&lt;li>Select resources by specifying the &amp;lt;group, version, kind&amp;gt; and a set of labels using ClusterResourcePlacement -&amp;gt; LabelSelector.
This selection propagates all resources that match the &amp;lt;group, version, kind&amp;gt; and label specified.&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Note:&lt;/strong> When a namespace is selected, all the namespace-scoped objects under this namespace are propagated to the
selected member clusters along with this namespace.&lt;/p></description></item><item><title>CRP Override Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementoverridden/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementoverridden/</guid><description>&lt;p>The status of the &lt;code>ClusterResourcePlacementOverridden&lt;/code> condition is set to &lt;code>false&lt;/code> when there is an Override API related issue.&lt;/p>
&lt;blockquote>
&lt;p>Note: To get more information, look into the logs for the overrider controller (includes
controller for &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/overrider/clusterresource_controller.go">ClusterResourceOverride&lt;/a> and
&lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/overrider/resource_controller.go">ResourceOverride&lt;/a>).&lt;/p>&lt;/blockquote>
&lt;h2 id="common-scenarios">Common scenarios&lt;/h2>
&lt;p>Instances where this condition may arise:&lt;/p>
&lt;ul>
&lt;li>The &lt;code>ClusterResourceOverride&lt;/code> or &lt;code>ResourceOverride&lt;/code> is created with an invalid field path for the resource.&lt;/li>
&lt;/ul>
&lt;h2 id="case-study">Case Study&lt;/h2>
&lt;p>In the following example, an attempt is made to override the cluster role &lt;code>secret-reader&lt;/code> that is being propagated by the &lt;code>ClusterResourcePlacement&lt;/code> to the selected clusters.
However, the &lt;code>ClusterResourceOverride&lt;/code> is created with an invalid path for the field within resource.&lt;/p></description></item><item><title>Getting started with Fleet using on-premises clusters</title><link>https://kubefleet-dev.github.io/website/docs/getting-started/on-prem/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/getting-started/on-prem/</guid><description>&lt;p>In this tutorial, you will try Fleet out using a few of your own Kubernetes clusters; Fleet can
help you manage workloads seamlessly across these clusters, greatly simplifying the experience
of day-to-day Kubernetes management.&lt;/p>
&lt;blockquote>
&lt;p>Note&lt;/p>
&lt;p>This tutorial assumes that you have some experience of performing administrative tasks for
Kubernetes clusters. If you are just gettings started with Kubernetes, or do not have much
experience of setting up a Kubernetes cluster, it is recommended that you follow the
&lt;a href="https://kubefleet-dev.github.io/website/docs/getting-started/kind/">Getting started with Fleet using Kind clusters&lt;/a> tutorial instead.&lt;/p></description></item><item><title>Using Affinity to Pick Clusters</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/affinities/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/affinities/</guid><description>&lt;p>This how-to guide discusses how to use affinity settings to fine-tune how Fleet picks clusters
for resource placement.&lt;/p>
&lt;p>Affinities terms are featured in the &lt;code>ClusterResourcePlacement&lt;/code> API, specifically the scheduling
policy section. Each affinity term is a particular requirement that Fleet will check against clusters;
and the fulfillment of this requirement (or the lack of) would have certain effect on whether
Fleet would pick a cluster for resource placement.&lt;/p>
&lt;p>Fleet currently supports two types of affinity terms:&lt;/p></description></item><item><title>CRP Work-Synchronization Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementworksynchronized/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementworksynchronized/</guid><description>&lt;p>The &lt;code>ClusterResourcePlacementWorkSynchronized&lt;/code> condition is false when the CRP has been recently updated but the associated work objects have not yet been synchronized with the changes.&lt;/p>
&lt;blockquote>
&lt;p>Note: In addition, it may be helpful to look into the logs for the &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/workgenerator/controller.go">work generator controller&lt;/a> to get more information on why the work synchronization failed.&lt;/p>&lt;/blockquote>
&lt;h2 id="common-scenarios">Common Scenarios&lt;/h2>
&lt;p>Instances where this condition may arise:&lt;/p>
&lt;ul>
&lt;li>The controller encounters an error while trying to generate the corresponding &lt;code>work&lt;/code> object.&lt;/li>
&lt;li>The enveloped object is not well formatted.&lt;/li>
&lt;/ul>
&lt;h3 id="case-study">Case Study&lt;/h3>
&lt;p>The CRP is attempting to propagate a resource to a selected cluster, but the work object has not been updated to reflect the latest changes due to the selected cluster has been terminated.&lt;/p></description></item><item><title>Using Topology Spread Constraints to Spread Resources</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/topology-spread-constraints/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/topology-spread-constraints/</guid><description>&lt;p>This how-to guide discusses how to use topology spread constraints to fine-tune how Fleet picks
clusters for resource placement.&lt;/p>
&lt;p>Topology spread constraints are features in the &lt;code>ClusterResourcePlacement&lt;/code> API, specifically
the scheduling policy section. Generally speaking, these constraints can help you spread
resources evenly across different groups of clusters in your fleet; or in other words, it
assures that Fleet will not pick too many clusters from one group, and too little from another.
You can use topology spread constraints to, for example:&lt;/p></description></item><item><title>CRP Work-Application Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementapplied/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementapplied/</guid><description>&lt;p>The &lt;code>ClusterResourcePlacementApplied&lt;/code> condition is set to &lt;code>false&lt;/code> when the deployment fails.&lt;/p>
&lt;blockquote>
&lt;p>Note: To get more information about why the resources are not applied, you can check the &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/work/apply_controller.go">apply work controller&lt;/a> logs.&lt;/p>&lt;/blockquote>
&lt;h2 id="common-scenarios">Common scenarios&lt;/h2>
&lt;p>Instances where this condition may arise:&lt;/p>
&lt;ul>
&lt;li>The resource already exists on the cluster and isn&amp;rsquo;t managed by the fleet controller.&lt;/li>
&lt;li>Another &lt;code>ClusterResourcePlacement&lt;/code> deployment is already managing the resource for the selected cluster by using a different apply strategy.&lt;/li>
&lt;li>The &lt;code>ClusterResourcePlacement&lt;/code> deployment doesn&amp;rsquo;t apply the manifest because of syntax errors or invalid resource configurations. This might also occur if a resource is propagated through an envelope object.&lt;/li>
&lt;/ul>
&lt;h2 id="investigation-steps">Investigation steps&lt;/h2>
&lt;ol>
&lt;li>Check &lt;code>placementStatuses&lt;/code>: In the &lt;code>ClusterResourcePlacement&lt;/code> status section, inspect the &lt;code>placementStatuses&lt;/code> to identify which clusters have the &lt;code>ResourceApplied&lt;/code> condition set to &lt;code>false&lt;/code> and note down their &lt;code>clusterName&lt;/code>.&lt;/li>
&lt;li>Locate the &lt;code>Work&lt;/code> Object in Hub Cluster: Use the identified &lt;code>clusterName&lt;/code> to locate the &lt;code>Work&lt;/code> object associated with the member cluster. Please refer to this &lt;a href="https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacement/#how-can-i-find-the-correct-work-resource-thats-associated-with-clusterresourceplacement">section&lt;/a> to learn how to get the correct &lt;code>Work&lt;/code> resource.&lt;/li>
&lt;li>Check &lt;code>Work&lt;/code> object status: Inspect the status of the &lt;code>Work&lt;/code> object to understand the specific issues preventing successful resource application.&lt;/li>
&lt;/ol>
&lt;h2 id="case-study">Case Study&lt;/h2>
&lt;p>In the following example, &lt;code>ClusterResourcePlacement&lt;/code> is trying to propagate a namespace that contains a deployment to two member clusters. However, the namespace already exists on one member cluster, specifically &lt;code>kind-cluster-1&lt;/code>.&lt;/p></description></item><item><title>Scheduler</title><link>https://kubefleet-dev.github.io/website/docs/concepts/scheduler/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/scheduler/</guid><description>&lt;p>The scheduler component is a vital element in Fleet workload scheduling. Its primary responsibility is to determine the
schedule decision for a bundle of resources based on the latest &lt;code>ClusterSchedulingPolicySnapshot&lt;/code>generated by the &lt;code>ClusterResourcePlacement&lt;/code>.
By default, the scheduler operates in batch mode, which enhances performance. In this mode, it binds a &lt;code>ClusterResourceBinding&lt;/code>
from a &lt;code>ClusterResourcePlacement&lt;/code> to multiple clusters whenever possible.&lt;/p>
&lt;h2 id="batch-in-nature">Batch in nature&lt;/h2>
&lt;p>Scheduling resources within a &lt;code>ClusterResourcePlacement&lt;/code> involves more dependencies compared with scheduling pods within
a deployment in Kubernetes. There are two notable distinctions:&lt;/p></description></item><item><title>Using Property-Based Scheduling</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/property-based-scheduling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/property-based-scheduling/</guid><description>&lt;p>This how-to guide discusses how to use property-based scheduling to produce scheduling decisions
based on cluster properties.&lt;/p>
&lt;blockquote>
&lt;p>Note&lt;/p>
&lt;p>The availability of properties depend on which (and if) you have a property provider
set up in your Fleet deployment. For more information, see the
&lt;a href="https://kubefleet-dev.github.io/website/concepts/properties">Concept: Property Provider and Cluster Properties&lt;/a>
documentation.&lt;/p>
&lt;p>It is also recommended that you read the
&lt;a href="https://kubefleet-dev.github.io/website/docs/how-tos/affinities/">How-To Guide: Using Affinity to Pick Clusters&lt;/a> first before following
instructions in this document.&lt;/p></description></item><item><title>CRP Availability Failure TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementavailable/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementavailable/</guid><description>&lt;p>The &lt;code>ClusterResourcePlacementAvailable&lt;/code> condition is &lt;code>false&lt;/code> when some of the resources are not available yet. We will place some of the detailed failure in the &lt;code>FailedResourcePlacement&lt;/code> array.&lt;/p>
&lt;blockquote>
&lt;p>Note: To get more information about why resources are unavailable check &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/work/apply_controller.go">apply work controller&lt;/a> logs.&lt;/p>&lt;/blockquote>
&lt;h2 id="common-scenarios">Common scenarios&lt;/h2>
&lt;p>Instances where this condition may arise:&lt;/p>
&lt;ul>
&lt;li>The member cluster doesn&amp;rsquo;t have enough resource availability.&lt;/li>
&lt;li>The deployment contains an invalid image name.&lt;/li>
&lt;/ul>
&lt;h2 id="case-study">Case Study&lt;/h2>
&lt;p>The example output below demonstrates a scenario where the CRP is unable to propagate a deployment to a member cluster due to the deployment having a bad image name.&lt;/p></description></item><item><title>Scheduling Framework</title><link>https://kubefleet-dev.github.io/website/docs/concepts/scheduling-framework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/scheduling-framework/</guid><description>&lt;p>The fleet scheduling framework closely aligns with the native &lt;a href="https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/">Kubernetes scheduling framework&lt;/a>,
incorporating several modifications and tailored functionalities.&lt;/p>
&lt;p>&lt;img src="https://kubefleet-dev.github.io/website/images/en/docs/concepts/scheduling-framework/scheduling-framework.jpg" alt="">&lt;/p>
&lt;p>The primary advantage of this framework lies in its capability to compile plugins directly into the scheduler. Its API
facilitates the implementation of diverse scheduling features as plugins, thereby ensuring a lightweight and maintainable
core.&lt;/p>
&lt;p>The fleet scheduler integrates three fundamental built-in plugin types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Topology Spread Plugin&lt;/strong>: Supports the TopologySpreadConstraints stipulated in the placement policy.&lt;/li>
&lt;li>&lt;strong>Cluster Affinity Plugin&lt;/strong>: Facilitates the Affinity clause of the placement policy.&lt;/li>
&lt;li>&lt;strong>Same Placement Affinity Plugin&lt;/strong>: Uniquely designed for the fleet, preventing multiple replicas (selected resources) from
being placed within the same cluster. This distinguishes it from Kubernetes, which allows multiple pods on a node.&lt;/li>
&lt;li>&lt;strong>Cluster Eligibility Plugin&lt;/strong>: Enables cluster selection based on specific status criteria.&lt;/li>
&lt;li>** Taint &amp;amp; Toleration Plugin**: Enables cluster selection based on taints on the cluster &amp;amp; tolerations on the ClusterResourcePlacement.&lt;/li>
&lt;/ul>
&lt;p>Compared to the Kubernetes scheduling framework, the fleet framework introduces additional stages for the pickN placement type:&lt;/p></description></item><item><title>Using Taints and Tolerations</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/taints-tolerations/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/taints-tolerations/</guid><description>&lt;p>This how-to guide discusses how to add/remove taints on &lt;code>MemberCluster&lt;/code> and how to add tolerations on &lt;code>ClusterResourcePlacement&lt;/code>.&lt;/p>
&lt;h2 id="adding-taint-to-membercluster">Adding taint to MemberCluster&lt;/h2>
&lt;p>In this example, we will add a taint to a &lt;code>MemberCluster&lt;/code>. Then try to propagate resources to the &lt;code>MemberCluster&lt;/code> using a &lt;code>ClusterResourcePlacement&lt;/code>
with &lt;strong>PickAll&lt;/strong> placement policy. The resources should not be propagated to the &lt;code>MemberCluster&lt;/code> because of the taint.&lt;/p>
&lt;p>We will first create a namespace that we will propagate to the member cluster,&lt;/p></description></item><item><title>ClusterResourcePlacementEviction TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementeviction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterresourceplacementeviction/</guid><description>&lt;p>This guide provides troubleshooting steps for issues related to placement eviction.&lt;/p>
&lt;p>An eviction object when created is ideally only reconciled once and reaches a terminal state. List of terminal states
for eviction are:&lt;/p>
&lt;ul>
&lt;li>Eviction is Invalid&lt;/li>
&lt;li>Eviction is Valid, Eviction failed to Execute&lt;/li>
&lt;li>Eviction is Valid, Eviction executed successfully&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> If an eviction object doesn&amp;rsquo;t reach a terminal state i.e. neither valid condition nor executed condition is
set it is likely due to a failure in the reconciliation process where the controller is unable to reach the api server.&lt;/p></description></item><item><title>Properties and Property Provides</title><link>https://kubefleet-dev.github.io/website/docs/concepts/properties/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/properties/</guid><description>&lt;p>This document explains the concepts of property provider and cluster properties in Fleet.&lt;/p>
&lt;p>Fleet allows developers to implement a property provider to expose arbitrary properties about
a member cluster, such as its node count and available resources for workload placement. Platforms
could also enable their property providers to expose platform-specific properties via Fleet.
These properties can be useful in a variety of cases: for example, administrators could monitor the
health of a member cluster using related properties; Fleet also supports making scheduling
decisions based on the property data.&lt;/p></description></item><item><title>Using the ClusterResourceOverride API</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/cluster-resource-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/cluster-resource-override/</guid><description>&lt;p>This guide provides an overview of how to use the Fleet &lt;code>ResourceOverride&lt;/code> API to override resources.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;code>ResourceOverride&lt;/code> is a Fleet API that allows you to modify or override specific attributes of
existing resources within your cluster. With ResourceOverride, you can define rules based on cluster
labels or other criteria, specifying changes to be applied to resources such as Deployments, StatefulSets, ConfigMaps, or Secrets.
These changes can include updates to container images, environment variables, resource limits, or any other configurable parameters.&lt;/p></description></item><item><title>ClusterStagedUpdateRun TSG</title><link>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterstagedupdaterun/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/troubleshooting/clusterstagedupdaterun/</guid><description>&lt;p>This guide provides troubleshooting steps for common issues related to Staged Update Run.&lt;/p>
&lt;blockquote>
&lt;p>Note: To get more information about why the scheduling fails, you can check the &lt;a href="https://github.com/kubefleet-dev/kubefleet/blob/main/pkg/controllers/updaterun/controller.go">updateRun controller&lt;/a> logs.&lt;/p>&lt;/blockquote>
&lt;h2 id="crp-status-without-staged-update-run">CRP status without Staged Update Run&lt;/h2>
&lt;p>When a &lt;code>ClusterResourcePlacement&lt;/code> is created with &lt;code>spec.strategy.type&lt;/code> set to &lt;code>External&lt;/code>, the rollout does not start immediately.&lt;/p>
&lt;p>A sample status of such &lt;code>ClusterResourcePlacement&lt;/code> is as follows:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>$ kubectl describe crp example-placement
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Status:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: found all cluster needed as specified by the scheduling policy, found &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> cluster&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: SchedulingPolicyFulfilled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: True
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: ClusterResourcePlacementScheduled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: There are still &lt;span style="color:#0000cf;font-weight:bold">2&lt;/span> cluster&lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>s&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span> in the process of deciding whether to roll out the latest resources or not
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: RolloutStartedUnknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: Unknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: ClusterResourcePlacementRolloutStarted
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Resource Index: &lt;span style="color:#0000cf;font-weight:bold">0&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Placement Statuses:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Cluster Name: member1
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: Successfully scheduled resources &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> placement in &lt;span style="color:#4e9a06">&amp;#34;member1&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>affinity score: 0, topology spread score: 0&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>: picked by scheduling policy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: Scheduled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: True
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: Scheduled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: In the process of deciding whether to roll out the latest resources or not
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: RolloutStartedUnknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: Unknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: RolloutStarted
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Cluster Name: member2
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Conditions:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: Successfully scheduled resources &lt;span style="color:#204a87;font-weight:bold">for&lt;/span> placement in &lt;span style="color:#4e9a06">&amp;#34;member2&amp;#34;&lt;/span> &lt;span style="color:#ce5c00;font-weight:bold">(&lt;/span>affinity score: 0, topology spread score: 0&lt;span style="color:#ce5c00;font-weight:bold">)&lt;/span>: picked by scheduling policy
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: Scheduled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: True
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: Scheduled
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Last Transition Time: 2025-03-12T23:01:32Z
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Message: In the process of deciding whether to roll out the latest resources or not
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Observed Generation: &lt;span style="color:#0000cf;font-weight:bold">1&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Reason: RolloutStartedUnknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Status: Unknown
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Type: RolloutStarted
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> Selected Resources:
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span> ...
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>Events: &amp;lt;none&amp;gt;
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;code>SchedulingPolicyFulfilled&lt;/code> condition indicates the CRP has been fully scheduled, while &lt;code>RolloutStartedUnknown&lt;/code> condition shows that the rollout has not started.&lt;/p></description></item><item><title>Safe Rollout</title><link>https://kubefleet-dev.github.io/website/docs/concepts/safe-rollout/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/safe-rollout/</guid><description>&lt;p>One of the most important features of Fleet is the ability to safely rollout changes across multiple clusters. We do
this by rolling out the changes in a controlled manner, ensuring that we only continue to propagate the changes to the
next target clusters if the resources are successfully applied to the previous target clusters.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>We automatically propagate any resource changes that are selected by a &lt;code>ClusterResourcePlacement&lt;/code> from the hub cluster
to the target clusters based on the placement policy defined in the &lt;code>ClusterResourcePlacement&lt;/code>. In order to reduce the
blast radius of such operation, we provide users a way to safely rollout the new changes so that a bad release
won&amp;rsquo;t affect all the running instances all at once.&lt;/p></description></item><item><title>Using the ResourceOverride API</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/resource-override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/resource-override/</guid><description>&lt;p>This guide provides an overview of how to use the Fleet &lt;code>ResourceOverride&lt;/code> API to override resources.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;code>ResourceOverride&lt;/code> is a Fleet API that allows you to modify or override specific attributes of
existing resources within your cluster. With ResourceOverride, you can define rules based on cluster
labels or other criteria, specifying changes to be applied to resources such as Deployments, StatefulSets, ConfigMaps, or Secrets.
These changes can include updates to container images, environment variables, resource limits, or any other configurable parameters.&lt;/p></description></item><item><title>Override</title><link>https://kubefleet-dev.github.io/website/docs/concepts/override/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/override/</guid><description>&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>The &lt;code>ClusterResourceOverride&lt;/code> and &lt;code>ResourceOverride&lt;/code> provides a way to customize resource configurations before they are propagated
to the target cluster by the &lt;code>ClusterResourcePlacement&lt;/code>.&lt;/p>
&lt;h2 id="difference-between-clusterresourceoverride-and-resourceoverride">Difference Between &lt;code>ClusterResourceOverride&lt;/code> And &lt;code>ResourceOverride&lt;/code>&lt;/h2>
&lt;p>&lt;code>ClusterResourceOverride&lt;/code> represents the cluster-wide policy that overrides the cluster scoped resources to one or more
clusters while &lt;code>ResourceOverride&lt;/code> will apply to resources in the same namespace as the namespace-wide policy.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Note:&lt;/strong> If a namespace is selected by the &lt;code>ClusterResourceOverride&lt;/code>, ALL the resources under the namespace are selected
automatically.&lt;/p></description></item><item><title>Using Envelope Objects to Place Resources</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/envelope-object/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/envelope-object/</guid><description>&lt;h2 id="propagating-resources-with-envelope-objects">Propagating Resources with Envelope Objects&lt;/h2>
&lt;p>This guide provides instructions on propagating a set of resources from the hub cluster to joined member clusters within an envelope object.&lt;/p>
&lt;h2 id="envelope-object-with-configmap">Envelope Object with ConfigMap&lt;/h2>
&lt;p>Currently, we support using a &lt;code>ConfigMap&lt;/code> as an envelope object by leveraging a fleet-reserved annotation.&lt;/p>
&lt;p>To designate a &lt;code>ConfigMap&lt;/code> as an envelope object, ensure that it contains the following annotation:&lt;/p>
&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#204a87;font-weight:bold">metadata&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">annotations&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#204a87;font-weight:bold">kubernetes-fleet.io/envelope-configmap&lt;/span>&lt;span style="color:#000;font-weight:bold">:&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline"> &lt;/span>&lt;span style="color:#4e9a06">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#f8f8f8;text-decoration:underline">
&lt;/span>&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="example-configmap-envelope-object">Example ConfigMap Envelope Object:&lt;/h3>
&lt;pre tabindex="0">&lt;code>apiVersion: v1
kind: ConfigMap
metadata:
 name: envelope-configmap
 namespace: app
 annotations:
 kubernetes-fleet.io/envelope-configmap: &amp;#34;true&amp;#34;
data:
 resourceQuota.yaml: |
 apiVersion: v1
 kind: ResourceQuota
 metadata:
 name: mem-cpu-demo
 namespace: app
 spec:
 hard:
 requests.cpu: &amp;#34;1&amp;#34;
 requests.memory: 1Gi
 limits.cpu: &amp;#34;2&amp;#34;
 limits.memory: 2Gi
 webhook.yaml: |
 apiVersion: admissionregistration.k8s.io/v1
 kind: MutatingWebhookConfiguration
 metadata:
 creationTimestamp: null
 labels:
 azure-workload-identity.io/system: &amp;#34;true&amp;#34;
 name: azure-wi-webhook-mutating-webhook-configuration
 webhooks:
 - admissionReviewVersions:
 - v1
 - v1beta1
 clientConfig:
 service:
 name: azure-wi-webhook-webhook-service
 namespace: app
 path: /mutate-v1-pod
 failurePolicy: Fail
 matchPolicy: Equivalent
 name: mutation.azure-workload-identity.io
 rules:
 - apiGroups:
 - &amp;#34;&amp;#34;
 apiVersions:
 - v1
 operations:
 - CREATE
 - UPDATE
 resources:
 - pods
 sideEffects: None
&lt;/code>&lt;/pre>&lt;h2 id="propagating-an-envelope-configmap-from-hub-cluster-to-member-cluster">Propagating an Envelope ConfigMap from Hub cluster to Member cluster:&lt;/h2>
&lt;p>We will now apply the example envelope object above on our hub cluster. Then we use a &lt;code>ClusterResourcePlacement&lt;/code> object to propagate the resource from hub to a member cluster named &lt;code>kind-cluster-1&lt;/code>.&lt;/p></description></item><item><title>Controlling How Fleet Handles Pre-Existing Resources</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/takeover/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/takeover/</guid><description>&lt;p>This guide provides an overview on how to set up Fleet&amp;rsquo;s takeover experience, which allows
developers and admins to choose what will happen when Fleet encounters a pre-existing resource.
This occurs most often in the Fleet adoption scenario, where a cluster just joins into a fleet and
the system finds out that the resources to place onto the new member cluster via the CRP API have
already been running there.&lt;/p></description></item><item><title>Enabling Drift Detection in Fleet</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/drift-detection/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/drift-detection/</guid><description>&lt;p>This guide provides an overview on how to enable drift detection in Fleet. This feature can help
developers and admins identify (and act upon) configuration drifts in their KubeFleet system,
which are often brought by temporary fixes, inadvertent changes, and failed automations.&lt;/p>
&lt;blockquote>
&lt;p>Before you begin&lt;/p>
&lt;p>The new drift detection experience is currently in preview.&lt;/p>
&lt;p>Note that the APIs for the new experience are only available in the Fleet v1beta1 API, not the v1 API. If you do not see the new APIs in command outputs, verify that you are explicitly requesting the v1beta1 API objects, as opposed to the v1 API objects (the default).&lt;/p></description></item><item><title>Using the ReportDiff Apply Mode</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/reportdiff/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/reportdiff/</guid><description>&lt;p>This guide provides an overview on how to use the &lt;code>ReportDiff&lt;/code> apply mode, which allows one to
easily evaluate how things will change in the system without the risk of incurring unexpected
changes. In this mode, Fleet will check for configuration differences between the hub cluster
resource templates and their corresponding resources on the member clusters, but will not
perform any apply op. This is most helpful in cases of experimentation and drift/diff analysis.&lt;/p></description></item><item><title>How to Roll Out and Roll Back Changes in Stage</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/staged-update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/staged-update/</guid><description>&lt;p>This how-to guide demonstrates how to use &lt;code>ClusterStagedUpdateRun&lt;/code> to rollout resources to member clusters in a staged manner and rollback resources to a previous version.&lt;/p>
&lt;h2 id="prerequisite">Prerequisite&lt;/h2>
&lt;p>&lt;code>ClusterStagedUpdateRun&lt;/code> CR is used to deploy resources from hub cluster to member clusters with &lt;code>ClusterResourcePlacement&lt;/code> (or CRP) in a stage by stage manner. This tutorial is based on a demo fleet environment with 3 member clusters:&lt;/p>
&lt;table>
 &lt;thead>
 &lt;tr>
 &lt;th>cluster name&lt;/th>
 &lt;th>labels&lt;/th>
 &lt;/tr>
 &lt;/thead>
 &lt;tbody>
 &lt;tr>
 &lt;td>member1&lt;/td>
 &lt;td>environment=canary, order=2&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>member2&lt;/td>
 &lt;td>environment=staging&lt;/td>
 &lt;/tr>
 &lt;tr>
 &lt;td>member3&lt;/td>
 &lt;td>environment=canary, order=1&lt;/td>
 &lt;/tr>
 &lt;/tbody>
&lt;/table>
&lt;p>To demonstrate the rollout and rollback behavior, we create a demo namespace and a sample configmap with very simple data on the hub cluster. The namespace with configmap will be deployed to the member clusters.&lt;/p></description></item><item><title>Staged Update</title><link>https://kubefleet-dev.github.io/website/docs/concepts/staged-update/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/staged-update/</guid><description>&lt;p>While users rely on the &lt;code>RollingUpdate&lt;/code> rollout strategy to safely roll out their workloads,
there is also a requirement for a staged rollout mechanism at the cluster level to enable more controlled and systematic continuous delivery (CD) across the fleet.
Introducing a staged update run feature would address this need by enabling gradual deployments, reducing risk, and ensuring greater reliability and consistency in workload updates across clusters.&lt;/p>
&lt;p>&lt;img src="https://kubefleet-dev.github.io/website/images/en/docs/concepts/staged-update/updaterun.jpg" alt="">&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>We introduce two new Custom Resources, &lt;code>ClusterStagedUpdateStrategy&lt;/code> and &lt;code>ClusterStagedUpdateRun&lt;/code>.&lt;/p></description></item><item><title>Eviction and Placement Disruption Budget</title><link>https://kubefleet-dev.github.io/website/docs/concepts/eviction-pdb/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/concepts/eviction-pdb/</guid><description>&lt;p>This document explains the concept of &lt;code>Eviction&lt;/code> and &lt;code>Placement Disruption Budget&lt;/code> in the context of the fleet.&lt;/p>
&lt;h2 id="overview">Overview&lt;/h2>
&lt;p>&lt;code>Eviction&lt;/code> provides a way to force remove resources from a target cluster once the resources have already been propagated from the hub cluster by a &lt;code>Placement&lt;/code> object.
&lt;code>Eviction&lt;/code> is considered as an voluntary disruption triggered by the user. &lt;code>Eviction&lt;/code> alone doesn&amp;rsquo;t guarantee that resources won&amp;rsquo;t be propagated to target cluster again by the scheduler.
The users need to use &lt;a href="../howtos/taint-toleration.md">taints&lt;/a> in conjunction with &lt;code>Eviction&lt;/code> to prevent the scheduler from picking the target cluster again.&lt;/p></description></item><item><title>Evicting Resources and Setting up Disruption Budgets</title><link>https://kubefleet-dev.github.io/website/docs/how-tos/eviction-disruption-budget/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://kubefleet-dev.github.io/website/docs/how-tos/eviction-disruption-budget/</guid><description>&lt;p>This how-to guide discusses how to create &lt;code>ClusterResourcePlacementEviction&lt;/code> objects and &lt;code>ClusterResourcePlacementDisruptionBudget&lt;/code> objects to evict resources from member clusters and protect resources on member clusters from voluntary disruption, respectively.&lt;/p>
&lt;h2 id="evicting-resources-from-member-clusters-using-clusterresourceplacementeviction">Evicting Resources from Member Clusters using ClusterResourcePlacementEviction&lt;/h2>
&lt;p>The &lt;code>ClusterResourcePlacementEviction&lt;/code> object is used to remove resources from a member cluster once the resources have already been propagated from the hub cluster.&lt;/p>
&lt;p>To successfully evict resources from a cluster, the user needs to specify:&lt;/p></description></item></channel></rss>